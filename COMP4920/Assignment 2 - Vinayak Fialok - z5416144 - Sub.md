> [!info] Question
> How might fairness, accountability, and transparency be achieved collectively by an automated decision - that is - by a decision outputted by an AI? In your answer, compare and contrast a causal account of explanation with at least one other account of explanation. Make explicit reference to the reading (and references therein) for Thursday of Week 4.
### 1. Introduction
Achieving the properties of accountability, fairness and transparency is essential to acceptable Automated Decision Making (ADM). We discuss how fairness and transparency may be jointly achieved for COMPAS, a recidivism prediction software. We argue that COMPAS may achieve transparency through simplification of its internal reasoning without diminishing accuracy. However, COMPAS does not have sufficient accuracy to result in fairness, particularly through the effect of automation bias.

First we define that achieving accountability is to have normatively salient reasoning (fairness) and accessible, understandable reasoning (transparency). Then, we discuss how Human decision making is able to achieve fairness and transparency through the belief-desire-intention model. We define reasoning as the process/steps in coming to a decision, and explanation as the accessible and understandable communication of reasoning. We then argue that we cannot achieve transparency through the translation of reasoning to explanation, and that reasoning itself must be inherently explicable. We explore ways of explanation, determining the deductive nomological and causal explanation are sufficient to explain decision. Next, we explore how rule-lists are explicable through deductive nomological and causal explanation. We show that COMPAS can be become transparent through simplification to rule-lists without losing accuracy. Next we argue that accuracy is not sufficient to prove moral salience, but rather the reasons underlying the decision should themselves be morally salient. We discuss how rule-lists may be shown to be morally-salient through Kantian ethics. Finally, we argue that the degree to which COMPAS is accurate, even with transparency, is not sufficient for fairness through considering ADM guidelines and cognitive bias.

## 2.1. Accountability, Fairness, and Transparency
First, let's define accountability, fairness and transparency. Borrowing from Grayson, we identify that **accountability requires reasons**. That is, an accountable decision is one that is reasonable. Reasonableness is a function of normatively rational salience (Grayson 2024 p5). For a reason to be normatively salient, a reason must be considered grounded in norms or morally "right" or "fair" within a given context. **So we derive our property of fairness as possessing reasons that are considered morally salient**. A decisions reasonableness also requires reportability, that is the reasons for a decision must also be understandable and accessible to its stakeholders. **So we have our last derived property of transparency which refers to the reportability of our decision.** Lets finally define that a decision that collectively achieves fairness, accountability and transparency is acceptable.

~~The question of *How might fairness, accountability, and transparency be achieved collectively by an automated decision* may then be phrased as "**How might an automated decision have reasons that are ethical in a transparent manner?**". To understand the notion of reasoning, lets explore how Grayson argues human decisions are reasonable and transparent.~~

### 2.2. Human Decision Making
**Through the belief-desire-intention model and causal explanation, humans are capable of making decisions that are fair and transparent, and as such accountable and acceptable.** Grayson explore human decision making through the a model mirroring the belief-desire-intention (BDI) agent model developed by Michael Bratman. Beliefs are the set of true statements understood by some agent, let's say Bob, such as "there is coffee in the fridge". Desires are an agent's goals, preferences and values, such as "Bob feels groggy and tired". Beliefs and desires are independent of each other. An intention is a plan, scheme or strategy formed in pursuit of an agent's desires, and based upon an agent's beliefs. The coexistence of the aforementioned belief and desire, and the lack of opposing desires such as "Bob wants to stay in bed", would culminate in the intention of "Bob will go to the kitchen and drink coffee". This line of reasoning naturally aligns itself with causal reasoning which may be phrased as "if Bob had not believed coffee lie in the fridge and Bob did not feel tired, then Bob would not have gone to get coffee". By explaining a decision using causal explanation in relation to beliefs and desires, one's decision is made transparent. By explaining morally salient beliefs and desires, one's decision is made fair.

### 2.3. COMPAS Recidivism Prediction
COMPAS (Correctional Offender Management Profiling and Alternative Sanctions) is an algorithm that predicts recidivism of convicted criminals. COMPAS aims to be used to assist judges in deciding the sentence of a convicted criminal by providing a prediction of the recidivism risk of said criminal. COMPAS uses at least 15 factors with multiple items (Center for Criminology and Public Policy Research 2010, p10) as of 2010, which has undoubtedly increased in the last decade.

### 2.4. Ways of Explanation
Grayson discusses 4 models of explanation of which we will discuss: the deductive-nomological model, the statistical model, and the causal model.

To explain some event or state $x$ through causal explanation is to specify the causes of $x$. *Counterfactual causal* explanation takes the form:
Where $c$ and $e$ are two distinct possible events, $e$ _causally depends_ on $c$ if and only if, if $c$ were to occur $e$ would occur; and if $c$ were not to occur $e$ would not occur. (Lewis 1973).
From our earlier example, event $c$ is "Bob feels tired and believes there is coffee in the fridge" and event e is "Bob goes to the fridge".

Statistical explanation is based upon the probability of event $c$ causing event $e$. This takes the form "given Bob feels tired and believes there is coffee in the fridge, bob will almost certainly go to the fridge" (adapted from Howel p125). Causal and statistical explanation are similar in that they identify a causal even $c$ and link $c$ to resulting event $e$. However, the difference between even $e$ occuring and "almost certainly occuring" is critical. Statistical explanation only explain the likeliness of $e$, which is not sufficient to explain the $e$ itself. **Although statistical explanation is not sufficient, statistical models may be used to determine causes for a causal explanation.** For example, the statistic that 95% of tired people with coffee nearby will go to drink coffee in conjunction with the elimination of confounding variables may be used to determine the causal relationship mentioned above.

Deductive nomological explanation involves starting from general laws and circumstances, and then showing "how that the statement describing the result can be validly inferred" (Hempel 1962 p100). For Bob we could say that "bob being tired and believing there is coffee in the fridge, and the general human law that individuals will take actions to remedy their fatigue implies that Bob will go to the fridge". In contrast, causal reasoning starts from the outcome event, and considers and refines the causes. Both causal and deductive explanation are sufficient to explain result $e$, we will explore the effectiveness of these models in attaining transparency next.

### 2.5. Reasoning to Explanation
**For an AI decision to be transparent, it must possess reasoning that tends easily tends towards explanation without significant translation (transparent reasoning).** Reasoning is the process in which a human or machine come to a decision. An explanation is the conveying of our reasoning to others. We can think of the process of conveying reasoning as translation from logic into the language of explanation, whether causal or otherwise.

For human decision making, there is little required translation. This is because the justification of a decision, that is the beliefs and desires of an agent are trivially explicable through causal reasoning as described with Bob.

Borrowing from Grayson's recidivism example, our judge has sentenced the defendant to 6 years with no parole through recommendation of COMPAS. Our defendant then asks for an explanation, "why has it been decided that I am given such a long punishment without parole?". The COMPAS creators know that it is not feasible to explain COMPAS's algorithm because it is too complex (and it is proprietary). The COMPAS creators decide to translate their reasoning, that is their algorithm, into an explanation through the means of a large language model or some new algorithm.

**A translation algorithm that can convert complex reasoning into accessible explanation requires that the resulting explanation is either not faithful, and so not transparent, or the algorithm can be designed through transparent reasoning.** Suppose after a translation step from complex logic into accessible explanation COMPAS pragmatically explains "you have offended once in the past and your demographic is deemed to be more likely to re-offend in the upcoming years". However, the defendant may then ask "how can I trust that this translation is true to the logic of the vastly complex COMPAS algorithm?". **Since neither the original reasoning nor the translation steps are transparent due to their complexity, COMPAS cannot be transparent even if the resulting explanation appears to be.** The latter case, that COMPAS may be simplified, is a promising approach to achieving transparency that we will discuss next.

### 3.1. Explicable reasoning
**Through rule-lists, which I will argue are transparent reasons, COMPAS may be made transparent without losing accuracy.**

As mentioned earlier, one approach to making COMPAS explicable is to make its reasoning explicable. An attempt of this was made by a research team from Berkely using the reasoning of "rule-lists" which are predictive models composed of if-then statements (Angelino et al 2018). Reasoning through rule lists is transparent because the reasoning for a decision is easily explicable through causal and deductive explanation. Let's see an example rule list below:

```python
if (age = 18 − 20) and (sex = male) then predict yes
else if (age = 21 − 23) and (priors = 2 − 3) then predict yes
else if (priors > 3) then predict yes
else predict no
```
(Angelino et al, 2018)

If our criminal from our aforementioned case was aged 19 and male we could explain causally with the precise if conditions that "because you are 19 and male, we have decided that you are at high risk of re-offending". Rule lists are also easy to convert to deductive explanation as "your age of 19 years, your male sex, and the general law this demographic is high risk proven through our data analysis, indicates you are at high risk of re-offending". Through rule-lists as above, researches created the CORELS algorithm which was "competitive with scores generated by the COMPAS algorithm" (Angelino et al 2018). So, through rule lists, CORELS and as such automated recidivism predication can become transparent.

Simplification of COMPAS into CORELS also does not diminish accuracy and so remains fair. "It is the accuracy of the COMPAS system that allows it to achieve fairness" (Grayson 2024 p7). The CORELS team found that "the accuracies of rule-lists learned by CORELS are competitive with scores generated by the black-box COMPAS algorithm" (Angelino et al 2024 p40). So, in the case of recidivism, CORELS achieves transparency through explicable rule-lists, whilst also maintaining accuracy, and so fairness, competitive with COMPAS. Though COMPAS's lack of transparency may be remedied without loss of accuracy, COMPAS's mediocre accuracy makes raises concerns regarding fairness.